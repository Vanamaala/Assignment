{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8db499-c08f-450a-96e7-cd2aa3990b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vanam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\vanam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               1408      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9729 (38.00 KB)\n",
      "Trainable params: 9729 (38.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##Create a white box neural network\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "def create_white_box_nn(input_shape):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "\n",
    "    # Hidden layers\n",
    "    model.add(layers.Dense(units=128, activation='relu'))\n",
    "    model.add(layers.Dense(units=64, activation='relu'))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Specify input shape (assuming a binary classification task with input features of size 10)\n",
    "input_shape = (10,)\n",
    "\n",
    "# Create the white-box neural network\n",
    "white_box_nn = create_white_box_nn(input_shape)\n",
    "\n",
    "# Display the model summary\n",
    "white_box_nn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83259b4d-25c9-410f-a2b7-298bfbf550b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Activation functions\n",
    "\n",
    "##1. Sigmoid function\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# Using numpy for vectorized operations\n",
    "def sigmoid_np(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c1b608a-1c8a-489b-a4f4-63ff945e9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "##2. tanh function\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def tanh(x):\n",
    "    return (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "\n",
    "# Using numpy for vectorized operations\n",
    "def tanh_np(x):\n",
    "    return np.tanh(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee55f8dd-daef-4969-b41c-422da6a42b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##3.RELU function\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "# Using numpy for vectorized operations\n",
    "import numpy as np\n",
    "\n",
    "def relu_np(x):\n",
    "    return np.maximum(0, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc7a394-9834-412a-881a-3295fe15dfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.86466472 -0.63212056  0.          1.          2.        ]\n"
     ]
    }
   ],
   "source": [
    "##4.ELU function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def elu(x, alpha=1.0):\n",
    "    return np.where(x >= 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "# Example usage\n",
    "x_values = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output_values = elu(x_values, alpha=1.0)\n",
    "print(output_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eca7c38f-5dc8-4582-94a4-fb34fe13ba65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02 -0.01  0.    1.    2.  ]\n"
     ]
    }
   ],
   "source": [
    "##5.PRELU function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class PReLU:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return np.where(x >= 0, x, self.alpha * x)\n",
    "\n",
    "# Example usage\n",
    "prelu_activation = PReLU(alpha=0.01)\n",
    "x_values = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output_values = prelu_activation(x_values)\n",
    "print(output_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dad13fba-958e-496e-b295-068fed8ad73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02 -0.01  0.    1.    2.  ]\n"
     ]
    }
   ],
   "source": [
    "##6.Leaky RELU function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x >= 0, x, alpha * x)\n",
    "\n",
    "# Example usage\n",
    "x_values = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output_values = leaky_relu(x_values, alpha=0.01)\n",
    "print(output_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eb61eb9-7cb4-4f72-bea8-b915280812ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.52016209 -1.11132754  0.          1.0507      2.1014    ]\n"
     ]
    }
   ],
   "source": [
    "##7.SELU function\n",
    "import numpy as np\n",
    "\n",
    "def selu(x, scale=1.0507, alpha=1.67326):\n",
    "    return scale * np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "# Example usage\n",
    "x_values = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output_values = selu(x_values)\n",
    "print(output_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "814dafc8-5760-422f-a0df-d993d38405e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.66666667 -0.5         0.          0.5         0.66666667]\n"
     ]
    }
   ],
   "source": [
    "##8.Softsign function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softsign(x):\n",
    "    return x / (1 + np.abs(x))\n",
    "\n",
    "# Example usage\n",
    "x_values = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output_values = softsign(x_values)\n",
    "print(output_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c88ec26-7bf8-4e1d-a62f-23eb8fc25d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12692801 0.31326169 0.69314718 1.31326169 2.12692801]\n"
     ]
    }
   ],
   "source": [
    "##9.Softplus\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softplus(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "# Example usage\n",
    "x_values = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output_values = softplus(x_values)\n",
    "print(output_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49586a86-fab6-41dc-9f10-7dc4e27acfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.16666666 0.33333333 0.5        0.66666667 0.83333334\n",
      " 1.        ]\n"
     ]
    }
   ],
   "source": [
    "##10.Hard sigmoid\n",
    "\n",
    "import numpy as np\n",
    "def hard_sigmoid(x):\n",
    "    return np.clip(0.16666667 * x + 0.5, 0, 1)\n",
    "# Example usage\n",
    "x_values = np.array([-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0])\n",
    "output_values = hard_sigmoid(x_values)\n",
    "print(output_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6b03004-bc7d-488f-9b3e-8b2233dbc77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.23840584 -0.26894142  0.          0.73105858  1.76159416]\n"
     ]
    }
   ],
   "source": [
    "##11.Swish function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def swish(x):\n",
    "    return x * (1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "# Example usage\n",
    "x_values = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output_values = swish(x_values)\n",
    "print(output_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db78f82f-b770-4c27-a1bd-0279bcd18eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.25250148 -0.30340146  0.          0.86509839  1.94395896]\n"
     ]
    }
   ],
   "source": [
    "##12.Mish function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def mish(x):\n",
    "    return x * np.tanh(np.log(1 + np.exp(x)))\n",
    "\n",
    "# Example usage\n",
    "x_values = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output_values = mish(x_values)\n",
    "print(output_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75ae1bbe-acc2-4d0a-8873-d3c300eec958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f'(x) = 2*x*cos(x**2)\n",
      "g'(x) = 2*x\n",
      "Chain Rule Result: 4*x**2*cos(x**2)\n"
     ]
    }
   ],
   "source": [
    "##Python code for chain rule\n",
    "\n",
    "import sympy as sp\n",
    "\n",
    "# Define symbols\n",
    "x = sp.symbols('x')\n",
    "\n",
    "# Define the inner and outer functions\n",
    "g = x**2\n",
    "f = sp.sin(g)\n",
    "\n",
    "# Compute the derivatives\n",
    "g_prime = sp.diff(g, x)\n",
    "f_prime = sp.diff(f, x)\n",
    "\n",
    "# Apply the chain rule\n",
    "chain_rule_result = f_prime.subs(g, x**2) * g_prime\n",
    "\n",
    "# Print the result\n",
    "print(\"f'(x) =\", f_prime)\n",
    "print(\"g'(x) =\", g_prime)\n",
    "print(\"Chain Rule Result:\", chain_rule_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b09bb55-45c9-4ec5-bde5-15bb4c69ce45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.14159479453221988\n",
      "Epoch 1000, Loss: 0.1234468957069047\n",
      "Epoch 2000, Loss: 0.11155406432451559\n",
      "Epoch 3000, Loss: 0.07778517861398411\n",
      "Epoch 4000, Loss: 0.03281496018045561\n",
      "Epoch 5000, Loss: 0.014799615014231389\n",
      "Epoch 6000, Loss: 0.008925515193170445\n",
      "Epoch 7000, Loss: 0.00624985817255592\n",
      "Epoch 8000, Loss: 0.004750368741015837\n",
      "Epoch 9000, Loss: 0.0037992877772672815\n",
      "Predictions:\n",
      "[[0.09620443]\n",
      " [0.92303163]\n",
      " [0.92312117]\n",
      " [0.06383034]]\n"
     ]
    }
   ],
   "source": [
    "##Function to perform back propagation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return 0.5 * np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mean_squared_error_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "    weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "    return weights_input_hidden, weights_hidden_output\n",
    "\n",
    "def forward_propagation(X, weights_input_hidden, weights_hidden_output):\n",
    "    hidden_layer_input = np.dot(X, weights_input_hidden)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "    return hidden_layer_output, output_layer_output\n",
    "\n",
    "def backward_propagation(X, y, hidden_layer_output, output_layer_output, \n",
    "                         weights_input_hidden, weights_hidden_output, learning_rate):\n",
    "    output_error = mean_squared_error_derivative(y, output_layer_output)\n",
    "    output_delta = output_error * sigmoid_derivative(output_layer_output)\n",
    "\n",
    "    hidden_error = output_delta.dot(weights_hidden_output.T)\n",
    "    hidden_delta = hidden_error * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    weights_hidden_output -= hidden_layer_output.T.dot(output_delta) * learning_rate\n",
    "    weights_input_hidden -= X.T.dot(hidden_delta) * learning_rate\n",
    "\n",
    "def train_neural_network(X, y, hidden_size, epochs, learning_rate):\n",
    "    input_size = X.shape[1]\n",
    "    output_size = 1\n",
    "\n",
    "    weights_input_hidden, weights_hidden_output = initialize_weights(input_size, hidden_size, output_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        hidden_layer_output, output_layer_output = forward_propagation(X, weights_input_hidden, weights_hidden_output)\n",
    "\n",
    "        backward_propagation(X, y, hidden_layer_output, output_layer_output, \n",
    "                             weights_input_hidden, weights_hidden_output, learning_rate)\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            loss = mean_squared_error(y, output_layer_output)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    return weights_input_hidden, weights_hidden_output\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "trained_weights_input_hidden, trained_weights_hidden_output = train_neural_network(X, y, hidden_size=4, epochs=10000, learning_rate=0.1)\n",
    "\n",
    "# Test the trained network\n",
    "_, predictions = forward_propagation(X, trained_weights_input_hidden, trained_weights_hidden_output)\n",
    "print(\"Predictions:\")\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22dbf79e-0b69-49c7-af30-f88036b11184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Best Fitness: -1.0095604657361734\n",
      "Iteration 1, Best Fitness: -1.0040480313601492\n",
      "Iteration 2, Best Fitness: -1.0067162945978299\n",
      "Iteration 3, Best Fitness: -1.0030817806282994\n",
      "Iteration 4, Best Fitness: -1.0082331569501404\n",
      "Iteration 5, Best Fitness: -1.0127640345587219\n",
      "Iteration 6, Best Fitness: -1.0152376468245525\n",
      "Iteration 7, Best Fitness: -1.0105918717655973\n",
      "Iteration 8, Best Fitness: -1.0070662742387426\n",
      "Iteration 9, Best Fitness: -1.0060008415633572\n",
      "Iteration 10, Best Fitness: -1.0076667425073977\n",
      "Iteration 11, Best Fitness: -1.0033938983630253\n",
      "Iteration 12, Best Fitness: -1.0024119274294954\n",
      "Iteration 13, Best Fitness: -1.003753841111412\n",
      "Iteration 14, Best Fitness: -1.006883584977868\n",
      "Iteration 15, Best Fitness: -1.0058747330445694\n",
      "Iteration 16, Best Fitness: -1.005778087792939\n",
      "Iteration 17, Best Fitness: -1.009414189492408\n",
      "Iteration 18, Best Fitness: -1.0075631799859663\n",
      "Iteration 19, Best Fitness: -1.0065880545620407\n",
      "Iteration 20, Best Fitness: -1.0026394995475252\n",
      "Iteration 21, Best Fitness: -1.0075436595499958\n",
      "Iteration 22, Best Fitness: -1.0132284957842634\n",
      "Iteration 23, Best Fitness: -1.0022259692951836\n",
      "Iteration 24, Best Fitness: -1.0114578606705142\n",
      "Iteration 25, Best Fitness: -1.0048418334812652\n",
      "Iteration 26, Best Fitness: -1.0104616321434179\n",
      "Iteration 27, Best Fitness: -1.0112179350550958\n",
      "Iteration 28, Best Fitness: -1.0096609159178387\n",
      "Iteration 29, Best Fitness: -1.0106447494238187\n",
      "Iteration 30, Best Fitness: -1.0059712094434503\n",
      "Iteration 31, Best Fitness: -1.0079478200737024\n",
      "Iteration 32, Best Fitness: -1.0177689717717895\n",
      "Iteration 33, Best Fitness: -1.002227331103292\n",
      "Iteration 34, Best Fitness: -1.0071729325375276\n",
      "Iteration 35, Best Fitness: -1.0069955073683536\n",
      "Iteration 36, Best Fitness: -1.0144408018616626\n",
      "Iteration 37, Best Fitness: -1.0054750031690298\n",
      "Iteration 38, Best Fitness: -1.0063179271946863\n",
      "Iteration 39, Best Fitness: -1.0072014632152337\n",
      "Iteration 40, Best Fitness: -1.0034000157674499\n",
      "Iteration 41, Best Fitness: -1.0105526581533473\n",
      "Iteration 42, Best Fitness: -1.0070903242228562\n",
      "Iteration 43, Best Fitness: -1.0081710942676345\n",
      "Iteration 44, Best Fitness: -1.0093010162708282\n",
      "Iteration 45, Best Fitness: -1.0058866658130077\n",
      "Iteration 46, Best Fitness: -1.007448017901682\n",
      "Iteration 47, Best Fitness: -1.0058670677875652\n",
      "Iteration 48, Best Fitness: -1.0068418258886118\n",
      "Iteration 49, Best Fitness: -1.0038227116762193\n",
      "Iteration 50, Best Fitness: -1.007335017448735\n",
      "Iteration 51, Best Fitness: -1.010352885154969\n",
      "Iteration 52, Best Fitness: -1.005642600042198\n",
      "Iteration 53, Best Fitness: -1.015917795865907\n",
      "Iteration 54, Best Fitness: -1.0083910465308443\n",
      "Iteration 55, Best Fitness: -1.0054988038308552\n",
      "Iteration 56, Best Fitness: -1.0099783556806567\n",
      "Iteration 57, Best Fitness: -1.0075610978171494\n",
      "Iteration 58, Best Fitness: -1.0096099082372318\n",
      "Iteration 59, Best Fitness: -1.009920001639025\n",
      "Iteration 60, Best Fitness: -1.0111067339820998\n",
      "Iteration 61, Best Fitness: -1.0092437512079306\n",
      "Iteration 62, Best Fitness: -1.0082068181130994\n",
      "Iteration 63, Best Fitness: -1.011849088358467\n",
      "Iteration 64, Best Fitness: -1.0049498070649439\n",
      "Iteration 65, Best Fitness: -1.0145457080582734\n",
      "Iteration 66, Best Fitness: -1.0045801796457992\n",
      "Iteration 67, Best Fitness: -1.0050505663317555\n",
      "Iteration 68, Best Fitness: -1.0122614247271948\n",
      "Iteration 69, Best Fitness: -1.0089521213556258\n",
      "Iteration 70, Best Fitness: -1.0065465621410605\n",
      "Iteration 71, Best Fitness: -1.0085244212347482\n",
      "Iteration 72, Best Fitness: -1.0110342560279215\n",
      "Iteration 73, Best Fitness: -1.0095874459115495\n",
      "Iteration 74, Best Fitness: -1.0189021288741666\n",
      "Iteration 75, Best Fitness: -1.0073582206744238\n",
      "Iteration 76, Best Fitness: -1.0055883623458068\n",
      "Iteration 77, Best Fitness: -1.0104805754506638\n",
      "Iteration 78, Best Fitness: -1.0052019651532036\n",
      "Iteration 79, Best Fitness: -1.004270171482208\n",
      "Iteration 80, Best Fitness: -1.0074349830533356\n",
      "Iteration 81, Best Fitness: -1.0051449374914216\n",
      "Iteration 82, Best Fitness: -1.0060901656484653\n",
      "Iteration 83, Best Fitness: -1.003322322753336\n",
      "Iteration 84, Best Fitness: -1.0051463566429117\n",
      "Iteration 85, Best Fitness: -1.0030828877200324\n",
      "Iteration 86, Best Fitness: -1.0028174195046085\n",
      "Iteration 87, Best Fitness: -1.0103869627733495\n",
      "Iteration 88, Best Fitness: -1.0072428212569462\n",
      "Iteration 89, Best Fitness: -1.0017730198753823\n",
      "Iteration 90, Best Fitness: -1.0072357705721868\n",
      "Iteration 91, Best Fitness: -1.0038629667119512\n",
      "Iteration 92, Best Fitness: -1.0118929946408963\n",
      "Iteration 93, Best Fitness: -1.0032583788642504\n",
      "Iteration 94, Best Fitness: -1.0095255328584691\n",
      "Iteration 95, Best Fitness: -1.0101111073960714\n",
      "Iteration 96, Best Fitness: -1.0033580132495126\n",
      "Iteration 97, Best Fitness: -1.0016964202313212\n",
      "Iteration 98, Best Fitness: -1.011768406150095\n",
      "Iteration 99, Best Fitness: -1.0045284427583658\n",
      "\n",
      "Final Predictions:\n",
      "[array([0.54401949]), array([0.54504171]), array([0.54625128]), array([0.54726829])]\n"
     ]
    }
   ],
   "source": [
    "##Back propagation for ant colony optimization\n",
    "import numpy as np\n",
    "\n",
    "# Define the task (e.g., XOR)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Neural network architecture\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "# ACO parameters\n",
    "num_ants = 5\n",
    "pheromone_matrix = np.ones((input_size * hidden_size + hidden_size * output_size,))\n",
    "\n",
    "# Define the sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define the neural network forward propagation\n",
    "def forward_propagation(weights, X):\n",
    "    input_hidden_weights = weights[:input_size * hidden_size].reshape(input_size, hidden_size)\n",
    "    hidden_output_weights = weights[input_size * hidden_size:].reshape(hidden_size, output_size)\n",
    "\n",
    "    hidden_layer_input = np.dot(X, input_hidden_weights)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "    output_layer_input = np.dot(hidden_layer_output, hidden_output_weights)\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "\n",
    "    return output_layer_output\n",
    "\n",
    "# Define the fitness function based on mean squared error\n",
    "def fitness(weights):\n",
    "    total_error = 0\n",
    "    for xi, target in zip(X, y):\n",
    "        output = forward_propagation(weights, xi)\n",
    "        total_error += np.mean((output - target) ** 2)\n",
    "    return -total_error  # Maximizing fitness, so negative of error\n",
    "\n",
    "# ACO-inspired exploration\n",
    "def explore_weights(pheromone_matrix):\n",
    "    weights = np.random.rand(len(pheromone_matrix))\n",
    "    weights = weights * pheromone_matrix\n",
    "    return weights / np.sum(weights)\n",
    "\n",
    "# Training loop\n",
    "num_iterations = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    for ant in range(num_ants):\n",
    "        # Explore the weight space using ACO\n",
    "        candidate_weights = explore_weights(pheromone_matrix)\n",
    "\n",
    "        # Evaluate fitness\n",
    "        fitness_candidate = fitness(candidate_weights)\n",
    "\n",
    "        # Update pheromone based on fitness\n",
    "        pheromone_matrix += learning_rate * (fitness_candidate - pheromone_matrix)\n",
    "\n",
    "    # Print the best fitness in each iteration\n",
    "    best_fitness = max(fitness(candidate_weights) for ant in range(num_ants))\n",
    "    print(f\"Iteration {iteration}, Best Fitness: {best_fitness}\")\n",
    "\n",
    "# Get the best weights from the pheromone matrix\n",
    "best_weights = explore_weights(pheromone_matrix)\n",
    "\n",
    "# Test the best weights\n",
    "predictions = [forward_propagation(best_weights, xi) for xi in X]\n",
    "print(\"\\nFinal Predictions:\")\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6aa3373f-fde4-46ff-8f26-5b112cade623",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 9 into shape (4,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Genetic algorithm loop\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_generations):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Evaluate fitness for each individual in the population\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     fitness_scores \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mfitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindividual\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindividual\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# Select parents based on fitness scores (roulette wheel selection)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     parents_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mrange\u001b[39m(population_size), size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, p\u001b[38;5;241m=\u001b[39mfitness_scores \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(fitness_scores))\n",
      "Cell \u001b[1;32mIn[31], line 56\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Genetic algorithm loop\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_generations):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Evaluate fitness for each individual in the population\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     fitness_scores \u001b[38;5;241m=\u001b[39m [\u001b[43mfitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindividual\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m individual \u001b[38;5;129;01min\u001b[39;00m population]\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# Select parents based on fitness scores (roulette wheel selection)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     parents_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mrange\u001b[39m(population_size), size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, p\u001b[38;5;241m=\u001b[39mfitness_scores \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(fitness_scores))\n",
      "Cell \u001b[1;32mIn[31], line 49\u001b[0m, in \u001b[0;36mfitness\u001b[1;34m(weights)\u001b[0m\n\u001b[0;32m     47\u001b[0m total_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xi, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, y):\n\u001b[1;32m---> 49\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     total_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((output \u001b[38;5;241m-\u001b[39m target) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mtotal_error\n",
      "Cell \u001b[1;32mIn[31], line 33\u001b[0m, in \u001b[0;36mforward_propagation\u001b[1;34m(weights, X)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_propagation\u001b[39m(weights, X):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Extract weights for input to hidden layer and hidden to output layer\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     input_hidden_weights \u001b[38;5;241m=\u001b[39m weights[:input_size \u001b[38;5;241m*\u001b[39m hidden_size]\u001b[38;5;241m.\u001b[39mreshape(input_size, hidden_size)\n\u001b[1;32m---> 33\u001b[0m     hidden_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Input to hidden layer\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     hidden_layer_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X, input_hidden_weights)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 9 into shape (4,1)"
     ]
    }
   ],
   "source": [
    "##Back propagation for genetic algorithm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the task (e.g., XOR)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Define neural network architecture\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "# Define genetic algorithm parameters\n",
    "population_size = 10\n",
    "mutation_rate = 0.1\n",
    "num_generations = 100\n",
    "\n",
    "# Initialize the population with random weights\n",
    "population = [np.random.randn((input_size + 1) * hidden_size + (hidden_size + 1) * output_size) for _ in range(population_size)]\n",
    "\n",
    "# Define the sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define the neural network forward propagation\n",
    "def forward_propagation(weights, X):\n",
    "    # Extract weights for input to hidden layer and hidden to output layer\n",
    "    input_hidden_weights = weights[:input_size * hidden_size].reshape(input_size, hidden_size)\n",
    "    hidden_output_weights = weights[input_size * hidden_size:].reshape(hidden_size, output_size)\n",
    "\n",
    "    # Input to hidden layer\n",
    "    hidden_layer_input = np.dot(X, input_hidden_weights)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "    # Hidden to output layer\n",
    "    output_layer_input = np.dot(hidden_layer_output, hidden_output_weights)\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "\n",
    "    return output_layer_output\n",
    "\n",
    "# Define the fitness function based on mean squared error\n",
    "def fitness(weights):\n",
    "    total_error = 0\n",
    "    for xi, target in zip(X, y):\n",
    "        output = forward_propagation(weights, xi)\n",
    "        total_error += np.mean((output - target) ** 2)\n",
    "    return -total_error  # Maximizing fitness, so negative of error\n",
    "\n",
    "# Genetic algorithm loop\n",
    "for generation in range(num_generations):\n",
    "    # Evaluate fitness for each individual in the population\n",
    "    fitness_scores = [fitness(individual) for individual in population]\n",
    "\n",
    "    # Select parents based on fitness scores (roulette wheel selection)\n",
    "    parents_indices = np.random.choice(range(population_size), size=2, p=fitness_scores / np.sum(fitness_scores))\n",
    "\n",
    "    # Crossover (single-point crossover)\n",
    "    crossover_point = np.random.randint(len(population[0]))\n",
    "    child1 = np.concatenate((population[parents_indices[0]][:crossover_point], population[parents_indices[1]][crossover_point:]))\n",
    "    child2 = np.concatenate((population[parents_indices[1]][:crossover_point], population[parents_indices[0]][crossover_point:]))\n",
    "\n",
    "    # Mutation\n",
    "    child1[np.random.rand(len(child1)) < mutation_rate] += np.random.randn(sum(np.random.rand(len(child1)) < mutation_rate))\n",
    "    child2[np.random.rand(len(child2)) < mutation_rate] += np.random.randn(sum(np.random.rand(len(child2)) < mutation_rate))\n",
    "\n",
    "    # Replace the least fit individuals with the new children\n",
    "    least_fit_index = np.argmin(fitness_scores)\n",
    "    population[least_fit_index] = child1\n",
    "    population[(least_fit_index + 1) % population_size] = child2\n",
    "\n",
    "    # Print the best fitness in each generation\n",
    "    best_fitness = max(fitness_scores)\n",
    "    print(f\"Generation {generation}, Best Fitness: {best_fitness}\")\n",
    "\n",
    "# Get the best individual (weights) from the final population\n",
    "best_weights = population[np.argmax(fitness_scores)]\n",
    "\n",
    "# Test the best individual\n",
    "predictions = [forward_propagation(best_weights, xi) for xi in X]\n",
    "print(\"\\nFinal Predictions:\")\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e5a9caf-f7ee-4a08-9627-e99a8ecf1ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.24688305216322626\n",
      "Epoch 1000, Loss: 0.1790691700409746\n",
      "Epoch 2000, Loss: 0.11833999071584195\n",
      "Epoch 3000, Loss: 0.0901214480083568\n",
      "Epoch 4000, Loss: 0.07902366765388376\n",
      "Epoch 5000, Loss: 0.07375055136729795\n",
      "Epoch 6000, Loss: 0.07075850718698443\n",
      "Epoch 7000, Loss: 0.0688048982746113\n",
      "Epoch 8000, Loss: 0.06732676417849096\n",
      "Epoch 9000, Loss: 0.06587313344193534\n",
      "\n",
      "Final Output:\n",
      "[[0.0608975 ]\n",
      " [0.92675102]\n",
      " [0.92694637]\n",
      " [0.48642503]]\n"
     ]
    }
   ],
   "source": [
    "##Back propagation for cultural algorithm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the task (e.g., XOR)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "\n",
    "# Define the sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward propagation\n",
    "    hidden_layer_input = np.dot(X, weights_input_hidden)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "\n",
    "    # Backpropagation\n",
    "    output_error = y - output_layer_output\n",
    "    output_delta = output_error * sigmoid_derivative(output_layer_output)\n",
    "\n",
    "    hidden_error = output_delta.dot(weights_hidden_output.T)\n",
    "    hidden_delta = hidden_error * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    # Update weights\n",
    "    weights_hidden_output += hidden_layer_output.T.dot(output_delta) * learning_rate\n",
    "    weights_input_hidden += X.T.dot(hidden_delta) * learning_rate\n",
    "\n",
    "    # Print the loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(output_error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Test the trained network\n",
    "hidden_layer_output = sigmoid(np.dot(X, weights_input_hidden))\n",
    "final_output = sigmoid(np.dot(hidden_layer_output, weights_hidden_output))\n",
    "print(\"\\nFinal Output:\")\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89702a96-4c4f-4de6-94ec-f2f43178c49f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
